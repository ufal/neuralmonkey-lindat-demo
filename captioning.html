<div class="boxTitle" data-toggle="collapse" data-target="#captioningEnBox">
    Image Captioning
</div>
<div class="boxContent collapse" id="captioningEnBox" style="text-align:center">
    <div style="background:#EEE;padding:10px,text-align:center;padding-bottom:10px">
        1. Select an image.
        <p align="center"><input type='file' id="imgFile" /></p>
        2. Select a rectangle you want to caption.
        <p align="center">
        <img src="" width="10" height="10" id="captioningImg" style="margin:20px;text-align=center" align="center"/></p>

        3. Choose the language of the caption.
        <div style="margin:20px">
            <input type="radio" name="captionLng" value="en" checked />
            <label>English</label>

            <!--<input type="radio" name="captionLng" value="de" />
            <label>German</label>-->

            <input type="radio" name="captionLng" value="cs" />
            <label>Czech</label>

        </div>

        <input type="button" class="btn btn-primary" id="captioningSubmit" value="Generate Caption" />
    </div>
    <div id="captioningResultBox" style="background:#ffffe0;padding:10px;margin:3px;text-align:center;font-size:20pt">
        </div>

<p style="text-align:left"> The image models are based on the paper <a
		href="http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf"><i>Show,
			Attend and Tell</i> by Kelvin Xu and his colleagues
		from University in Montreal from 2015</a>. The model uses a
pre-trained image representation from a convolutional network.
A recurrent neural network then generates the caption word by word, in each
step focusing on only some regions of the image,
using the attention mechanism.
</p>

<p style="text-align:left">The English model was trained on the <a
   href="http://web.engr.illinois.edu/~bplumme2/Flickr30kEntities/">Flickr30k dataset</a>, which consists of more than
30,000 images from <a
   href="https://www.flickr.com">Flickr</a>, each accompanied by 5 independent
human descriptions summarizing the contents of the image. The Czech model was
trained on the <a
	href="https://github.com/multi30k/dataset">Multi30k dataset</a>, which contains a translation of one of the English
captions for each image.</p>

</div>
