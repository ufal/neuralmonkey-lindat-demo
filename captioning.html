<div class="boxTitle" data-toggle="collapse" data-target="#captioningEnBox">
    Image Captioning
</div>
<div class="boxContent collapse" id="captioningEnBox" style="text-align:center">
    <div style="background:#EEE;padding:10px,text-align:center">
        1. Select an image.
        <p align="center"><input type='file' id="imgFile" /></p>
        2. Select a rectangle you want to caption.
        <p align="center">
        <img src="" width="10" height="10" id="captioningImg" style="margin:20px;text-align=center" align="center"/></p>

        3. Choose the language of the caption.
        <div style="margin:20px">
            <input type="radio" name="captionLng" value="en" checked />
            <label>English</label>

            <!--<input type="radio" name="captionLng" value="de" />
            <label>German</label>-->

            <input type="radio" name="captionLng" value="cs" />
            <label>Czech</label>

        </div>

        <input type="button" class="btn btn-primary" id="captioningSubmit" value="Generate Caption" />
    </div>
    <div id="captioningResultBox" style="background:#ffffe0;padding:10px;margin:3px;text-align:center;font-size:20pt">
        </div>

<p style="text-align:left"> The image models are based on the paper Show, Attend and Tell by Kelvin Xu and
his colleagues from University in Montreal from 2015. The model uses a
pre-trained image representation from a convolutional network.
A recurrent neural network then generates the caption word by word, in each
step focusing on only some regions of the image,
using the attention mechanism.
</p>

<p style="text-align:left">The English model was trained on the Flickr30k dataset, which consists of more than
30,000 images from Flickr, each accompanied by 5 independent
human descriptions summarizing the contents of the image. The Czech model was
trained on the Multi30k dataset, which contains a translation of one of the English
captions for each image.</p>

</div>
