<div class="boxTitle" data-toggle="collapse" data-target="#captioningEnBox">
    Image Captioning
</div>
<div class="boxContent collapse" id="captioningEnBox" style="text-align:center">
    <div style="background:#EEE;padding:10px,text-align:center">
        1. Select an image.
        <p align="center"><input type='file' id="imgFile" /></p>
        2. Select a rectangle you want caption.
        <p align="center">
        <img src="" width="10" height="10" id="captioningImg" style="margin:20px;text-align=center" align="center"/></p>

        3. Choose the language of the caption.
        <div style="margin:20px">
            <input type="radio" name="captionLng" value="en" checked />
            <label>English</label>

            <!--<input type="radio" name="captionLng" value="de" />
            <label>German</label>-->

            <input type="radio" name="captionLng" value="cs" />
            <label>Czech</label>

        </div>

        <input type="button" class="btn btn-primary" id="captioningSubmit" value="Generate Caption" />
    </div>
    <div id="captioningResultBox" style="background:#ffffe0;padding:10px;margin:3px;text-align:center;font-size:20pt">
        </div>

<p style="text-align:left"> The image models are based on paper Show, Attend and Tell by Kelvin Xu and
his colleagues from University in Montreal from 2015. The model uses a
pre-trained image representation from a convolutional network which is
processed by a recurrent neural network that in each steps focuses on specific
regions on the image using the attention mechanism and produces an output
word.</p>

<p style="text-align:left">The English model trained on the Flick30k dataset which consists more than
30,000 images from Flickr and each of them is accompanied with 5 independent
human description summarizing what is in the image. The Czech model was on the
part of the Multi30k dataset which consist contains translation of the English
captions.</p>

</div>
